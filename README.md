# Derm1M: A Millionâ€‘Scale Visionâ€‘Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology

[![ArXiv](https://img.shields.io/badge/arXiv-2503.14911-b31b1b)](https://arxiv.org/abs/2503.14911)
[![License](https://img.shields.io/badge/License-CC%20BYâ€“NC%204.0-green)](#license)
[![Downloads](https://img.shields.io/badge/Data-Coming%20Soon-blue)](#data-access)

<div align="center">
  <img src="overview.png" alt="Overview of Derm1M Dataset" width="80%"/>
</div>

## âœ¨ TL;DR

**Derm1M** brings **1,029,761 dermatologistâ€‘curated imageâ€“text pairs**â€”257Ã— more than any previous dermatology visionâ€‘language corpusâ€”covering **390 skin conditions** and **130 clinical concepts** organised in a fourâ€‘level expert ontology. The datasetâ€™s rich contextual captions (meanâ€¯=â€¯41â€¯tokens) include Fitzpatrick skinâ€‘tone metadata and other clinical details, enabling explainable multimodal learning, zeroâ€‘/fewâ€‘shot diagnosis, crossâ€‘modal retrieval and visual question answering in realistic settings.

## ğŸ“¦ Whatâ€™s inside?

| Aspect                                                      | Derm1M                                                                                                  |
| :---------------------------------------------------------- | :------------------------------------------------------------------------------------------------------ |
| Total pairs                                                 | **1,029,761**                                                                                           |
| Skin conditions                                             | **390** (4 hierarchy levels)                                                                            |
| Clinical concepts                                           | **130**                                                                                                 |
| Avg. caption length                                         | **41 words**                                                                                            |
| Image sources                                               | YouTube (51â€¯k vids), PubMed (566â€¯k figs), Medical forums (49â€¯k posts), Public datasets, Teaching slides |
| Fitzpatrick tone metadata \| Present (Iâ€“VI, expertâ€‘audited) |                                                                                                         |
| Ontology                                                    | Structured JSONÂ + graph (disease â‡Œ concept â‡Œ finding)                                                   |

## ğŸ”‘ Key Features

* **Scale that matters** â€“ 1â€¯M+ multimodal examples enable training CLIPâ€‘style encoders from scratch or adapting large visionâ€‘language models.
* **Comprehensive coverage** â€“ 390 conditions span inflammatory, infectious, neoplastic and genodermatoses, mirroring realâ€‘world prevalence.
* **Rich context & concepts** â€“ Descriptions embed history, symptoms, anatomic sites, Fitzpatrick tone and 130 physicianâ€‘understandable concepts for trustworthy reasoning.
* **Expertâ€‘curated ontology** â€“ Four boardâ€‘certified dermatologists built a hierarchical knowledge graph to supervise multiâ€‘granular learning.
* **Benchmarkâ€‘ready splits** â€“ Standard train/val/test splits plus 8 downstream datasets for plugâ€‘andâ€‘play evaluation.

## ğŸ—ï¸ Repository Layout

```text
dataset_root/
â”œâ”€â”€ images/                # JPEG & PNG images
â”œâ”€â”€ captions.jsonl         # text + meta per image
â”œâ”€â”€ ontology.json          # disease & concept hierarchy
â”œâ”€â”€ splits/                # train/val/test indices
â””â”€â”€ docs/
    â”œâ”€â”€ dataset_card.md
    â””â”€â”€ LICENSE
```

## ğŸš€ Preâ€‘trained Models: **DermLIP**

We provide two CLIPâ€‘style checkpoints trained from scratch on **Derm1M**:

| Model ID            | Vision Encoder | Text Encoder | Zeroâ€‘shot Avgâ€  | R\@10 Iâ†’T (holdâ€‘out) |
| :------------------ | :------------- | :----------- | :------------: | :------------------: |
| **DermLIPâ€‘B/16**    | ViTâ€‘B/16       | GPT77        |     56.1â€¯%     |        40.7â€¯%        |
| **DermLIPâ€‘PanDerm** | PanDermâ€‘B      | PMB256       |   **58.8â€¯%**   |      **59.9â€¯%**      |

â€ Average accuracy across HAM10000, Fitzpatrick17K, PADâ€‘UFESâ€‘20 and Daffodil datasets (TableÂ 2) with retrieval metrics from TableÂ 4. îˆ€fileciteîˆ‚turn1file15îˆ‚turn1file6îˆ

Weights, inference scripts and data loaders will be released upon paper acceptance.

## ğŸ“Š Key Benchmarks

* **Zeroâ€‘shot disease classification (4 datasets):** 58.8â€¯% accuracy, outperforming BiomedCLIP by **â‰ˆâ€¯14.7â€¯pp** îˆ€fileciteîˆ‚turn3file2îˆ.
* **Fewâ€‘shot (1â€¯% labels) linear probing:** 58.6â€¯% accuracy vs. MONET 53.0â€¯% (**+5.6â€¯pp**) îˆ€fileciteîˆ‚turn2file2îˆ.
* **Crossâ€‘modal retrieval â€“ Derm1M holdâ€‘out:** Recall\@10 (Iâ†’T)â€¯59.9â€¯%, **3.6â€¯Ã—** BiomedCLIP îˆ€fileciteîˆ‚turn3file14îˆ.
* **Crossâ€‘modal retrieval â€“ SkinCAP external:** Recall\@10 (Iâ†’T)â€¯20.2â€¯%, **+6.0â€¯pp** over MONET îˆ€fileciteîˆ‚turn3file14îˆ.

*All numbers correspond to the DermLIP PanDermâ€‘B/PMB256 configuration reported in TablesÂ 2â€“4 of the paper.*

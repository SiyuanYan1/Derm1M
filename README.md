# Derm1M: A Millionâ€‘Scale Visionâ€‘Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology

[![ArXiv](https://img.shields.io/badge/arXiv-2503.14911-b31b1b)](https://arxiv.org/abs/2503.14911)
[![License](https://img.shields.io/badge/License-CC%20BYâ€“NC%204.0-green)](#license)
[![Downloads](https://img.shields.io/badge/Data-Coming%20Soon-blue)](#data-access)

<div align="center">
  <img src="overview.png" alt="Overview of Derm1M Dataset" width="80%"/>
</div>

## âœ¨ TL;DR

**Derm1M** brings **1,029,761 dermatologistâ€‘curated imageâ€“text pairs**â€”257Ã— more than any previous dermatology visionâ€‘language corpusâ€”covering **390 skin conditions** and **130 clinical concepts** organised in a fourâ€‘level expert ontology. The datasetâ€™s rich contextual descriptions (41â€¯words on average) and balanced representation across Fitzpatrick skin tones unlock new possibilities for explainable multimodal learning, zero-/fewâ€‘shot diagnosis, crossâ€‘modal retrieval and visual question answering in realâ€‘world clinical settings.

## ğŸ“¦ Whatâ€™s inside?

| Aspect              | Derm1M                                                                                                  |
| :------------------ | :------------------------------------------------------------------------------------------------------ |
| Total pairs         | **1,029,761**                                                                                           |
| Skin conditions     | **390** (4 hierarchy levels)                                                                            |
| Clinical concepts   | **130**                                                                                                 |
| Avg. caption length | **41 words**                                                                                            |
| Image sources       | YouTube (51â€¯k vids), PubMed (566â€¯k figs), Medical forums (49â€¯k posts), Public datasets, Teaching slides |
| Skinâ€‘tone balance   | Fitzpatrick Iâ€“VI, expertâ€‘audited                                                                        |
| Ontology            | Structured JSONÂ + graph (disease â‡Œ concept â‡Œ finding)                                                   |

## ğŸ”‘ Key Features

* **Scale that matters** â€“ 1â€¯M+ multimodal examples enable training CLIPâ€‘style encoders from scratch or adapting large visionâ€‘language models.
* **Comprehensive coverage** â€“ 390 conditions span inflammatory, infectious, neoplastic and genodermatoses, mirroring realâ€‘world prevalence.
* **Rich context & concepts** â€“ Descriptions embed history, symptoms, anatomic sites, Fitzpatrick tone and 130 physicianâ€‘understandable concepts for trustworthy reasoning.
* **Expertâ€‘curated ontology** â€“ Four boardâ€‘certified dermatologists built a hierarchical knowledge graph to supervise multiâ€‘granular learning.
* **Benchmarkâ€‘ready splits** â€“ Standard train/val/test splits plus 8 downstream datasets for plugâ€‘andâ€‘play evaluation.

## ğŸ—ï¸ Repository Layout

```text
dataset_root/
â”œâ”€â”€ images/                # JPEG & PNG images
â”œâ”€â”€ captions.jsonl         # text + meta per image
â”œâ”€â”€ ontology.json          # disease & concept hierarchy
â”œâ”€â”€ splits/                # train/val/test indices
â””â”€â”€ docs/
    â”œâ”€â”€ dataset_card.md
    â””â”€â”€ LICENSE
```

## ğŸš€ Preâ€‘trained Models: **DermLIP**

We release CLIPâ€‘like checkpoints (ViTâ€‘B/16, ViTâ€‘L/14, ViTâ€‘H/14) trained on Derm1M:

| Model            | Zeroâ€‘shot ISICâ€‘2018 | Fewâ€‘shot SkinConâ€‘48 | Crossâ€‘modal R\@1 |
| :--------------- | :-----------------: | :-----------------: | :--------------: |
| BioMedCLIP       |         71.2        |         42.3        |       25.6       |
| **DermLIPâ€‘B/16** |       **81.0**      |       **56.8**      |     **37.9**     |
| DermLIPâ€‘L/14     |         83.4        |         58.7        |       40.4       |

Weights and inference scripts will be published once the paper is accepted.

## ğŸ“Š Key Benchmarks (DermLIP vs. SOTA)

| Task                                                                                        |   Metric  | DermLIP (PanDermâ€‘B/PMB256) |  Best Prior SOTA  |                âœš / Î”               |
| :------------------------------------------------------------------------------------------ | :-------: | :------------------------: | :---------------: | :--------------------------------: |
| **Zeroâ€‘shot disease classification** (avg. HAM10000, PADâ€‘UFESâ€‘20, Fitzpatrick17K, Daffodil) |    Acc.   |         **58.8â€¯%**         | BiomedCLIPÂ 45.0â€¯% | **+13.8â€¯pp** îˆ€fileciteîˆ‚turn0file0îˆ |
| **Fewâ€‘shot (1â€¯% labels) linear probing**                                                    |    Acc.   |         **58.6â€¯%**         |    MONETÂ 53.0â€¯%   |  **+5.6â€¯pp** îˆ€fileciteîˆ‚turn0file0îˆ |
| **Fullâ€‘shot (100â€¯% labels) linear probing**                                                 |    Acc.   |         **79.3â€¯%**         |  PanDermâ€‘LÂ 80.2â€¯% |    âˆ’0.9â€¯pp îˆ€fileciteîˆ‚turn0file0îˆ   |
| **Crossâ€‘modal retrieval** (Derm1M holdâ€‘out)                                                 | R\@10 Iâ†’T |         **59.9â€¯%**         | BiomedCLIPÂ 16.6â€¯% |   **Ã—3.6** îˆ€fileciteîˆ‚turn0file0îˆ   |
| **Crossâ€‘modal retrieval** (SkinCAP external)                                                | R\@10 Iâ†’T |         **20.2â€¯%**         |    MONETÂ 14.2â€¯%   |  **+6.0â€¯pp** îˆ€fileciteîˆ‚turn0file0îˆ |

*Numbers extracted from TablesÂ 2â€“4 of the paper. All baselines use similarâ€‘sized ViTâ€‘B encoders unless noted.*

## ğŸ’¾ Data Access

The dataset is undergoing final ethical review and will be released for **nonâ€‘commercial research under CCÂ BYâ€‘NCâ€‘4.0**. Join the [waitâ€‘list](https://forms.gle/derm1mâ€‘access) or watch this repo for updates.

## ğŸ“ Getting Started

```bash
git clone https://github.com/SiyuanYan1/Derm1M.git
conda env create -f environment.yml
python demo_clip_inference.py --image examples/lesion.jpg --topk 5
```

See the [`docs`](docs/) folder for the data loader, ontology parser, and downstream task examples.

## ğŸ“š Citation

If you find our work useful, please cite:

```bibtex
@misc{yan2025derm1m,
  title        = {Derm1M: A Millionâ€‘Scale Visionâ€‘Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology},
  author       = {Siyuan Yan and Ming Hu and Yiwen Jiang and Xieji Li and Hao Fei and Philipp Tschandl and Harald Kittler and Zongyuan Ge},
  year         = {2025},
  eprint       = {2503.14911},
  archivePrefix= {arXiv},
  primaryClass = {cs.CV},
  url          = {https://arxiv.org/abs/2503.14911}
}
```

## ğŸ›¡ï¸ License

Derm1M is released under the **Creative Commons Attributionâ€‘NonCommercial 4.0 International** license. Commercial use requires separate permission.

## ğŸ™ Acknowledgements

We thank the ISIC archive, DermNetNZ, medical educators on YouTube, and the volunteer dermatologists whose expertise made Derm1M possible.
